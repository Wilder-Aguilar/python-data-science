# -*- coding: utf-8 -*-
"""Extracción_Datos_WebScraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y3Znk46OTpNp20lc7H7dC_BwFbUZS4xL

<h1>Extracción de datos bursátiles mediante web scraping</h1>

No todos los datos bursátiles están disponibles a través de la API en esta tarea; utilizarás el web scraping para obtener datos financieros. Se te evaluarán tus resultados.
Extraerás y compartirás datos históricos de una página web utilizando la biblioteca BeautifulSoup.
"""

!pip install pandas
!pip install requests
!pip install bs4
!pip install html5lib
!pip install lxml
!pip install plotly

import pandas as pd
import requests
from bs4 import BeautifulSoup

"""En Python, puedes ignorar las advertencias utilizando el módulo warnings. Puedes utilizar la función filterwarnings para filtrar o ignorar mensajes de advertencia específicos o categorías."""

import warnings
# Ignore all warnings
warnings.filterwarnings("ignore", category=FutureWarning)

"""## Ejemplo de uso del web scraping para extraer datos bursátiles

Extraeremos los datos bursátiles de Netflix. [https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html).

<center>
    
#### En este ejemplo, utilizamos el sitio web de Yahoo Finanzas y buscamos extraer datos de Netflix.

</center>
    <br>

  <img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/Images/netflix.png"> </center>
  
<center> Fig:- Tabla que necesitamos extraer </center>

En la siguiente página web tenemos una tabla con columnas con los siguientes nombres (Fecha, Apertura, Máximo, Mínimo, Cierre, Cierre ajustado, Volumen), de la que debemos extraer las siguientes columnas:  
* Date

* Open  

* High

* Low

* Close

* Volume

# Pasos para extraer los datos
1. Envía una solicitud HTTP a la página web utilizando la biblioteca requests.
2. Analiza el contenido HTML de la página web utilizando BeautifulSoup.
3. Identifica las etiquetas HTML que contienen los datos que deseas extraer.
4. Utiliza los métodos de BeautifulSoup para extraer los datos de las etiquetas HTML.
5. Imprime los datos extraídos.

### Paso 1: Enviar una solicitud HTTP a la página web

Utilizarás la biblioteca de solicitudes para enviar una solicitud HTTP a la página web.
"""

url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html"

"""El método requests.get() toma una URL como primer argumento, que especifica la ubicación del recurso que se va a recuperar. En este caso, el valor de la variable url se pasa como argumento al método requests.get(), ya que se va a almacenar la URL de una página web en una variable url.

Se utiliza el método .text para extraer el contenido HTML como una cadena con el fin de que sea legible.
"""

data  = requests.get(url).text
print(data)

"""### Paso 2: Analizar el contenido HTML

<hr>
<hr>
<center>

# ¿Qué es el análisis sintáctico?
En términos sencillos, el análisis sintáctico se refiere al proceso de analizar una cadena de texto o una estructura de datos, normalmente siguiendo un conjunto de reglas o gramática, para comprender su estructura y significado.
El análisis sintáctico consiste en descomponer un fragmento de texto o datos en sus componentes o elementos individuales y, a continuación, analizar dichos componentes para extraer la información deseada o comprender sus relaciones y significados. </center>
<hr>
<hr>

A continuación, tomarás el contenido HTML sin procesar de una página web o una cadena de código HTML que debe analizarse y transformarse en un formato estructurado y jerárquico que pueda analizarse y manipularse más fácilmente en Python. Esto se puede hacer utilizando una biblioteca de Python llamada <b>Beautiful Soup</b>.

## Analizar los datos utilizando la biblioteca BeautifulSoup
* Crear un nuevo objeto BeautifulSoup.
<br>
<br>
<b>Nota: </b>Para crear un objeto BeautifulSoup en Python, es necesario pasar dos argumentos a su constructor:

1. El contenido HTML o XML que desea analizar como una cadena.
2. El nombre del analizador que desea utilizar para analizar el contenido HTML o XML. Este argumento es opcional y, si no especifica ningún analizador, BeautifulSoup utilizará el analizador HTML predeterminado incluido en la biblioteca.
En este laboratorio estamos utilizando el analizador «html5lib».
"""

soup = BeautifulSoup(data, 'html.parser')

"""### Paso 3: Identificar las etiquetas HTML

Como se ha indicado anteriormente, la página web consta de una tabla, por lo que extraeremos el contenido de la página web HTML y convertiremos la tabla en un marco de datos.

Crearás un marco de datos vacío utilizando la función <b> pd.DataFrame() </b> con las siguientes columnas:
* "Date"
* "Open"
* "High"
* "Low"
* "Close"
* "Volume"
"""

netflix_data = pd.DataFrame(columns=["Date", "Open", "High", "Low", "Close", "Volume"])

"""<hr>
<hr>
<center>

### Trabajando en una tabla HTML  </center>
<br>

Estas son las siguientes etiquetas que se utilizan al crear tablas HTML.

* &lt;table&gt;: Esta etiqueta es una etiqueta raíz que se utiliza para definir el inicio y el final de la tabla. Todo el contenido de la tabla se encuentra entre estas etiquetas.


* &lt;tr&gt;: Esta etiqueta se utiliza para definir una fila de la tabla. Cada fila de la tabla se define dentro de esta etiqueta.

* &lt;td&gt;:  Esta etiqueta se utiliza para definir una celda de tabla. Cada celda de la tabla se define dentro de esta etiqueta. Puede especificar el contenido de la celda entre las etiquetas de apertura y cierre <td>.

* &lt;th&gt;: Esta etiqueta se utiliza para definir una celda de encabezado en la tabla. La celda de encabezado se utiliza para describir el contenido de una columna o fila. Por defecto, el texto dentro de una etiqueta <th> aparece en negrita y centrado.

* &lt;tbody&gt;: Este es el contenido principal de la tabla, que se define mediante la etiqueta <tbody>. Contiene una o más filas de elementos <tr>.

<hr>
<hr>

### Paso 4: Utiliza un método de BeautifulSoup para extraer datos.

Utilizaremos <b>find()</b> and <b>find_all()</b> métodos del objeto BeautifulSoup para localizar el cuerpo de la tabla y la fila de la tabla, respectivamente, en el HTML.
   * The <i>find() method </i> Devolverá el contenido de una etiqueta concreta..
   * The <i>find_all()</i> el método devuelve una lista de todas las etiquetas coincidentes en el HTML.
"""

# Primero aislamos el cuerpo de la tabla que contiene toda la información
# A continuación, recorremos cada fila y buscamos todos los valores de las columnas de cada fila
for row in soup.find("tbody").find_all('tr'):
    col = row.find_all("td")
    date = col[0].text
    Open = col[1].text
    high = col[2].text
    low = col[3].text
    close = col[4].text
    adj_close = col[5].text
    volume = col[6].text

    # Finalmente, añadimos los datos de cada fila a la tabla.
    netflix_data = pd.concat([netflix_data,pd.DataFrame({"Date":[date], "Open":[Open], "High":[high], "Low":[low], "Close":[close], "Adj Close":[adj_close], "Volume":[volume]})], ignore_index=True)

"""### Paso 5: Imprimir los datos extraídos

Ahora podemos imprimir el marco de datos utilizando la función head() o tail().
"""

netflix_data.head()

"""# Extracción de datos utilizando la biblioteca `pandas`

También podemos utilizar la función `read_html` de la biblioteca pandas y utilizar la URL para extraer datos.

<center>

## ¿Qué es read_html en la biblioteca pandas?

`pd.read_html(url)` es una función proporcionada por la biblioteca pandas en Python que se utiliza para extraer tablas de páginas web HTML. Toma una URL como entrada y devuelve una lista de todas las tablas encontradas en la página web.
</center>
"""

read_html_pandas_data = pd.read_html(url)

"""O bien, puede convertir el objeto BeautifulSoup en una cadena."""

read_html_pandas_data = pd.read_html(str(soup))

"""Como solo hay una tabla en la página, basta con tomar la primera tabla de la lista devuelta."""

netflix_dataframe = read_html_pandas_data[0]

netflix_dataframe.head()

"""# Ejercicio: utilizar el webscraping para extraer datos bursátiles.

Utiliza la biblioteca `requests` para descargar la página web. [https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html). Guarde el texto de la respuesta como una variable llamada `html_data`.
"""

url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html"

html_data  = requests.get(url).text
print(html_data)

"""Analiza los datos HTML utilizando `beautiful_soup` con html.parser."""

soup = BeautifulSoup(html_data, 'html.parser')

"""<b>Pregunta 1:</b> ¿Cuál es el contenido del atributo title?"""

title_page = soup.title.string

# Imprimir el resultado
print(f"El contenido del atributo title es: {title_page}")

"""Utilizando BeautifulSoup, extraiga la tabla con los precios históricos de las acciones y guárdela en un marco de datos denominado «amazon_data». El marco de datos debe tener las columnas Fecha, Apertura, Máximo, Mínimo, Cierre, Cierre ajustado y Volumen. Rellene cada variable con los datos correctos de la lista «col»."""

amazon_data = pd.DataFrame(columns=["Date", "Open", "High", "Low", "Close", "Volume"])

for row in soup.find("tbody").find_all("tr"):
    col = row.find_all("td")
    date = col[0].text
    Open = col[1].text
    high = col[2].text
    low = col[3].text
    close = col[4].text
    adj_close = col[5].text
    volume = col[6].text

    amazon_data = pd.concat([amazon_data, pd.DataFrame({"Date":[date], "Open":[Open], "High":[high], "Low":[low], "Close":[close], "Adj Close":[adj_close], "Volume":[volume]})], ignore_index=True)

"""Imprime las primeras cinco filas del marco de datos `amazon_data` que has creado."""

amazon_data.head()

"""<b>Pregunta 2:</b> ¿Cómo se llaman las columnas del marco de datos?"""

print(amazon_data.columns)

"""Pregunta 3: ¿Cuál es el valor de apertura de la última fila del marco de datos amazon_data?"""

# Accede a la última fila del DataFrame (iloc[-1])
# y luego a la columna 'Open'
valor_apertura_ultima_fila = amazon_data.iloc[-1]['Open']

print(f"El valor de apertura de la última fila es: {valor_apertura_ultima_fila}")